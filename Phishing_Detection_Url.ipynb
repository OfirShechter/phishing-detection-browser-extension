{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQmVau26VXlm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JcUPi9aVmF7"
      },
      "outputs": [],
      "source": [
        "\n",
        "urls_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/URL dataset.csv'\n",
        "only_phishing_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/Phishing URLs.csv'\n",
        "improve_dataset_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/URL-own_dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCAcxsFVV7HX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(urls_path)\n",
        "    phishing_df = pd.read_csv(only_phishing_path)\n",
        "    final_merged_df = pd.read_csv(improve_dataset_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found\")\n",
        "    exit()\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse the CSV file\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCefgrGZWFAr"
      },
      "outputs": [],
      "source": [
        "# prompt: lowercase headers in dfs and the type column value\n",
        "\n",
        "# Lowercase headers\n",
        "df.columns = [col.lower() for col in df.columns]\n",
        "phishing_df.columns = [col.lower() for col in phishing_df.columns]\n",
        "\n",
        "# Lowercase values in the 'type' column\n",
        "df['type'] = df['type'].str.lower()\n",
        "phishing_df['type'] = phishing_df['type'].str.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2s44qSXWXjB"
      },
      "outputs": [],
      "source": [
        "new_all_url_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/better-dataset/ALL-phishing-links.lst'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tiSFGkuVCfD"
      },
      "outputs": [],
      "source": [
        "# Load the list into a Python list\n",
        "with open(new_all_url_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    urls = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
        "\n",
        "# Preview\n",
        "print(f\"Total URLs: {len(urls)}\")\n",
        "print(\"First 5 URLs:\")\n",
        "for url in urls[:5]:\n",
        "    print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ32Nf_rR87E"
      },
      "outputs": [],
      "source": [
        "# prompt: add to 'final_merged_df' all phishing urls (stored as list string in 'urls').  final_merged_df has two colums- 'url' (string url) and 'type' (legitimate or phishing)\n",
        "\n",
        "import pandas as pd\n",
        "# Create a DataFrame from the list of URLs\n",
        "new_urls_df = pd.DataFrame({'url': urls, 'type': 'phishing'})\n",
        "\n",
        "# Concatenate the new DataFrame with the existing DataFrame\n",
        "final_merged_df = pd.concat([final_merged_df, new_urls_df], ignore_index=True)\n",
        "final_merged_df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYrP9VZP79WR"
      },
      "outputs": [],
      "source": [
        "final_merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74fx4InTWYQt"
      },
      "outputs": [],
      "source": [
        "# prompt: extend df by adding phishing urls from 'https://openphish.com/feed.txt', legitamite urls from http://5000best.com/websites/\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "try:\n",
        "    # Fetch phishing URLs\n",
        "    phishing_response = requests.get('https://openphish.com/feed.txt')\n",
        "    phishing_response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "    phishing_urls = phishing_response.text.strip().split('\\n')\n",
        "    phishing_df_new = pd.DataFrame({'url': phishing_urls, 'type': 'phishing'})\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching URLs: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "#Display the first few rows of the updated dataframe\n",
        "phishing_df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg0OxRMCR9FZ"
      },
      "outputs": [],
      "source": [
        "phishing_df_new.iloc[495]['url']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goj26clSSWOF"
      },
      "source": [
        "# create final_merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTeJ-U33Yno9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "base_url = \"http://5000best.com/websites/\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "all_websites = []\n",
        "page = 1\n",
        "\n",
        "while True:\n",
        "    url = base_url if page == 1 else f\"{base_url}{page}\"\n",
        "    # print(f\"Scraping page {page}...\")\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        # print(\"No more pages or failed to load.\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    links = soup.select(\"table#ttable a.n\")\n",
        "\n",
        "    # If no links found, we've reached the end\n",
        "    if not links:\n",
        "        break\n",
        "\n",
        "    websites = [a.get_text(strip=True) for a in links]\n",
        "    all_websites.extend(websites)\n",
        "\n",
        "    page += 1\n",
        "\n",
        "print(f\"\\nTotal websites found: {len(all_websites)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWF927nlFsa_"
      },
      "outputs": [],
      "source": [
        "# prompt: get from csv the values under 'Domain' (csv path- /content/drive/MyDrive/Ofir/CyberAI/Phishing/better-dataset/majestic_million.csv) save to dataframe (url, type: legitimate)\n",
        "\n",
        "import pandas as pd\n",
        "majestic_million_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/better-dataset/majestic_million.csv'\n",
        "\n",
        "try:\n",
        "    majestic_df = pd.read_csv(majestic_million_path)\n",
        "    # Select the 'Domain' column and create a new DataFrame\n",
        "    legitimate_domains_df = majestic_df[['Domain', \"TLD\"]]\n",
        "    # Rename the 'Domain' column to 'url'\n",
        "    legitimate_domains_df = legitimate_domains_df.rename(columns={'Domain': 'url'})\n",
        "    # Add a 'type' column with the value 'legitimate'\n",
        "    legitimate_domains_df['type'] = 'legitimate'\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {majestic_million_path}\")\n",
        "    legitimate_domains_df = pd.DataFrame(columns=['url', 'type']) #create empty df to avoid error\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse the CSV file at {majestic_million_path}\")\n",
        "    legitimate_domains_df = pd.DataFrame(columns=['url', 'type']) #create empty df to avoid error\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "    legitimate_domains_df = pd.DataFrame(columns=['url', 'type']) #create empty df to avoid error\n",
        "\n",
        "\n",
        "print(legitimate_domains_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5OA6lkXbfdf"
      },
      "outputs": [],
      "source": [
        "# prompt: add https:// protocol  to each site in all_websites\n",
        "\n",
        "# Add https:// protocol to each site in all_websites\n",
        "all_websites_https = [\"https://\" + site for site in all_websites]\n",
        "print(all_websites_https[:5]) #Print first 5 to verify\n",
        "all_websites_https_www = [\"https://www.\" + site for site in all_websites]\n",
        "print(all_websites_https_www[:5]) #Print first 5 to verify\n",
        "# concat to one list\n",
        "legit_urls = all_websites_https + all_websites_https_www"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2gLtMsAbdWQ"
      },
      "outputs": [],
      "source": [
        "legit_df_new = pd.DataFrame({'url': legit_urls, 'type': 'legitimate'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3u9t98vYihQ"
      },
      "outputs": [],
      "source": [
        "# merged_df = pd.concat([df, phishing_df_new, legit_df_new], ignore_index=True)\n",
        "merged_df = pd.concat([phishing_df_new, legit_df_new], ignore_index=True)\n",
        "merged_df.drop_duplicates(subset=['url'], keep='first', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_6xa5xqdmUS"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbUv_nRNgG1F"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "\n",
        "nltk.download('words') # Download the words dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Na0utxXTdo63"
      },
      "outputs": [],
      "source": [
        "# prompt: take top 50 domains from 'all_websites' and write code that generate phishing examples that tries to resample them\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "\n",
        "\n",
        "# Assuming 'all_websites' is defined as in your previous code\n",
        "\n",
        "top_domains = all_websites[:100]\n",
        "tld_top = ['com', 'top', 'xyz', 'sbs', 'cn', 'shop', 'ru', 'online', 'info', 'site', 'cfd', 'net', 'org', 'bond', 'cc', 'pro', 'life', 'buzz', 'pl', 'click']\n",
        "p_tld_top = [0.1, 0.1, 0.1, 0.1, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375, 0.0375 ]\n",
        "def generate_phishing_example(domainWithTld):\n",
        "    #\n",
        "    domain, tld = domainWithTld.split('.')\n",
        "    # Method 1: Add random subdomains\n",
        "    subdomains = [\"login\", \"secure\", \"account\", \"signin\", \"verify\", \"update\"]\n",
        "    random_subdomain = random.choice(subdomains)\n",
        "    phishing_url_1 = f\"{domain}.{random_subdomain}.{random.choice(words.words())}\"\n",
        "\n",
        "    # Method 2: Replace with similar-looking characters\n",
        "    similar_chars = {\"o\": \"0\", \"l\": \"1\", \"i\": \"1\", \"e\": \"3\", \"a\": \"4\", \"s\": \"5\"}  # Add more as needed\n",
        "    modified_domain = \"\"\n",
        "    for char in domain:\n",
        "      modified_domain += similar_chars.get(char, char)\n",
        "    phishing_url_2 = f\"{modified_domain}\"\n",
        "\n",
        "    # Method 3: Add random characters at the beginning or end\n",
        "    random_chars = \"\".join(random.choice(\"abcdefghijklmnopqrstuvwxyz0123456789\") for i in range(3))\n",
        "    phishing_url_3 = f\"{random_chars}{domain}\"\n",
        "    phishing_url_4 = f\"{domain}{random_chars}\"\n",
        "    phishing_url_5 = f\"{domain}{random_chars}.{random.choice(words.words())}\"\n",
        "    phishing_url_6 = f\"{random_chars}{domain}{random_chars}.{random.choice(words.words())}\"\n",
        "    phishing_url_7 = f\"{domain}.{tld}@{random.choice(words.words())}\"\n",
        "    #Return a list of phishing variations\n",
        "    result = [[f\"https://{s}\", f\"https://www.{s}\" ] for s in [phishing_url_1, phishing_url_2, phishing_url_3, phishing_url_4, phishing_url_5, phishing_url_6, phishing_url_7]]\n",
        "    result_with_tld = [[f\"{d1}.{np.random.choice(tld_top, p=p_tld_top)}\", f\"{d2}.{np.random.choice(tld_top, p=p_tld_top)}\"] for [d1, d2] in result]\n",
        "    return [item for sublist in result_with_tld for item in sublist]\n",
        "\n",
        "phishing_examples = []\n",
        "for domainWithTld in top_domains:\n",
        "  # print(domainWithTld)\n",
        "  if domainWithTld.count('.') == 1:\n",
        "    r = generate_phishing_example(domainWithTld)\n",
        "    # print(r)\n",
        "    phishing_examples.extend(r)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qnp3iOfKZot"
      },
      "outputs": [],
      "source": [
        "# prompt: take top 50 domains from 'all_websites' and write code that generate phishing examples that tries to resample them\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "\n",
        "\n",
        "# Assuming 'all_websites' is defined as in your previous code\n",
        "\n",
        "top_domains = all_websites[:-1]\n",
        "\n",
        "def generate_legitimate_example(domainWithTld, tld):\n",
        "    #\n",
        "    domain = domainWithTld[:-(len(tld)+1)]\n",
        "    # Method 1: Add random subdomains\n",
        "    subdomains = [\"login\", \"secure\", \"account\", \"signin\", \"verify\", \"update\"]\n",
        "    random_subdomain = random.choice(subdomains)\n",
        "    good_url_1 = f\"https://{random_subdomain}.{random.choice(words.words())}.{domain}\"\n",
        "    good_url_2 = f\"https://{random_subdomain}.{domain}\"\n",
        "    good_url_3 = f\"https://{random.choice(words.words())}.{domain}\"\n",
        "\n",
        "    return [f\"{s}.{tld}\" for s in [good_url_1, good_url_2, good_url_3]]\n",
        "\n",
        "\n",
        "# legitimate_examples = []\n",
        "# for domainWithTld in top_domains:\n",
        "#   # print(domainWithTld)\n",
        "#   if domainWithTld.count('.') == 1:\n",
        "#       legitimate_examples.extend(generate_legitimate_example(domainWithTld))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MEINCPNKPtn"
      },
      "outputs": [],
      "source": [
        "\"google.com\"[:-(len(\"com\")+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiR_X1DUQ-VK"
      },
      "outputs": [],
      "source": [
        "# prompt: take first 100,000 rows from 'legitimate_domains_df'\n",
        "\n",
        "legitimate_domains_df_first_100k = legitimate_domains_df.head(100000)\n",
        "legitimate_domains_df_first_100k.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El2lshSpaGbp"
      },
      "outputs": [],
      "source": [
        "!pip install swifter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JRgAoGBazzq"
      },
      "outputs": [],
      "source": [
        "word_list = words.words()\n",
        "subdomains = [\"login\", \"secure\", \"account\", \"signin\", \"verify\", \"update\"]\n",
        "\n",
        "def fast_generate(domain, tld):\n",
        "    d = domain[:-(len(tld)+1)]\n",
        "    w1, w2 = random.choice(word_list), random.choice(word_list)\n",
        "    sd = random.choice(subdomains)\n",
        "\n",
        "    urls = [\n",
        "        f\"https://{sd}.{w1}.{d}.{tld}\",\n",
        "        f\"https://{sd}.{d}.{tld}\",\n",
        "        f\"https://{w2}.{d}.{tld}\",\n",
        "        f\"https://{d}.{tld}\",\n",
        "        f\"https://www.{d}.{tld}\"\n",
        "    ]\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2yl3f9ObZhw"
      },
      "outputs": [],
      "source": [
        "# Vectorized creation of all phishing-lookalike URLs\n",
        "generated_urls = []\n",
        "expanded_legitimate_df = legitimate_domains_df\n",
        "\n",
        "for domain, tld in zip(expanded_legitimate_df['url'], expanded_legitimate_df['TLD']):\n",
        "    generated_urls.append(fast_generate(domain, tld))\n",
        "\n",
        "# Create the exploded dataframe\n",
        "expanded_legitimate_df['url'] = generated_urls\n",
        "expanded_legitimate_df = expanded_legitimate_df[['url', 'type']]\n",
        "expanded_legitimate_df = expanded_legitimate_df.explode('url', ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5iev7tFI6FK"
      },
      "outputs": [],
      "source": [
        "expanded_legitimate_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBIyuMj0iWWQ"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame for the phishing examples\n",
        "phishing_examples_df = pd.DataFrame({'url': phishing_examples, 'type': 'phishing'})\n",
        "legitimate_examples_df = pd.DataFrame({'url': legitimate_examples, 'type': 'legitimate'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tby2e8PukLch"
      },
      "outputs": [],
      "source": [
        "legitimate_examples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKECNctmfLIL"
      },
      "outputs": [],
      "source": [
        "# Concatenate with the existing merged_df\n",
        "final_merged_df = pd.concat([merged_df, phishing_examples_df, legitimate_examples_df], ignore_index=True)\n",
        "\n",
        "# Remove duplicates again after adding new phishing examples\n",
        "final_merged_df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
        "final_merged_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__fEmk1ocQg1"
      },
      "outputs": [],
      "source": [
        "# Concatenate with the existing merged_df\n",
        "final_merged_df = pd.concat([expanded_legitimate_df, new_urls_df], ignore_index=True)\n",
        "\n",
        "# Remove duplicates again after adding new phishing examples\n",
        "final_merged_df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
        "final_merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrQhZobQOMro"
      },
      "outputs": [],
      "source": [
        "final_merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfE1j2LLPOsJ"
      },
      "outputs": [],
      "source": [
        "# prompt: save df to csv at '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/URL-improved dataset.csv'\n",
        "\n",
        "final_merged_df.to_csv('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/URL-own_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdoX6LLPQckD"
      },
      "outputs": [],
      "source": [
        "# prompt: show in pi-graph legitimate and phishing in 'final_merged_df'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Count the occurrences of each type\n",
        "type_counts = final_merged_df['type'].value_counts()\n",
        "\n",
        "# Create the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribution of Legitimate and Phishing URLs')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk0HJZUqrP97"
      },
      "source": [
        "# URL Feature extract and create models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH6vxFg3rXHd"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRsB9IzPrT5L"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "tokens = enc.encode(\"https://example.com/path?q=test\")\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLda-3WprWNF"
      },
      "outputs": [],
      "source": [
        "dec = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "dec.decode(tokens[6:7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfTkdruprn8p"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "import re\n",
        "\n",
        "def extract_url_features(url):\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        hostname = parsed.hostname\n",
        "        hostname_parts = parsed.hostname.split('.') if parsed.hostname else []\n",
        "        domain = hostname_parts[-2] if len(hostname_parts) >= 2 else ''\n",
        "        tld = hostname_parts[-1] if hostname_parts else ''\n",
        "        subdomains = hostname_parts[:-2]\n",
        "        # Check for domain-like patterns in the path\n",
        "        domain_like_pattern = r\"(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}\"\n",
        "        has_domain_like_path = bool(re.search(domain_like_pattern, parsed.path))\n",
        "\n",
        "\n",
        "        if (parsed.scheme != 'https') and (parsed.scheme != 'http'):\n",
        "          print(f'################### UNIQE SCHEMA: {parsed.scheme}')\n",
        "        features = {\n",
        "            \"protocol\": True if parsed.scheme == 'https' else False,\n",
        "            \"hasAuth\": bool(parsed.username or parsed.password),\n",
        "            \"subdomains\": subdomains,\n",
        "            \"domain\": domain,\n",
        "            \"tld\": tld,\n",
        "            \"port\": bool(parsed.port),\n",
        "            # \"path\": parsed.path,\n",
        "            # \"query\": parsed.query,\n",
        "            # \"pathLength\": len(parsed.path),\n",
        "            # \"queryLength\": len(parsed.query),\n",
        "            # \"urlLength\": len(url),\n",
        "            # \"hasAtOrTildSymbol\": \"@\" in url or \"~\" in url,\n",
        "            \"hasDoubleSlash\": \"//\" in url[8:],  # Check for double slash after position 8\n",
        "            # \"hyphenCount\": url.count(\"-\") if has_domain_like_path else (parsed.hostname.count('-') if parsed.hostname else 0.0),\n",
        "            \"numbersInSubdomains\": sum(c.isdigit() for part in subdomains for c in part),\n",
        "            # \"hasDomainLikeInPath\": has_domain_like_path,\n",
        "            \"numOfSubdomains\": len(subdomains)\n",
        "        }\n",
        "        return features\n",
        "    except ValueError:\n",
        "        print(f\"Invalid URL: {url}\")\n",
        "        return None  # Or raise an exception if you prefer\n",
        "\n",
        "# Example usage\n",
        "url = \"https://user:password@www.example.co.uk:8080/path/to/page?q=query#fragment\"\n",
        "features = extract_url_features(url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV7p4Jems9nN"
      },
      "outputs": [],
      "source": [
        "# features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YZOnYjEtxk2"
      },
      "outputs": [],
      "source": [
        "# prompt: apply tiktoken on strings and convert all values in object to tensors\n",
        "\n",
        "import torch\n",
        "\n",
        "# Example usage\n",
        "# url = \"https://user:password@www.example.co.uk:8080/path/to/page?q=query#fragment\"\n",
        "# features = extract_url_features(url)\n",
        "\n",
        "\n",
        "# Convert feature values to tensors\n",
        "def convert_features_to_tensor(features):\n",
        "    tensor_features = {}\n",
        "    for key, value in features.items():\n",
        "        if isinstance(value, str):\n",
        "            tokens = enc.encode(value)\n",
        "            tensor_features[key] = torch.tensor(tokens, dtype=torch.long)\n",
        "        elif isinstance(value, list):\n",
        "            tensor_features[key] = [torch.tensor(enc.encode(item), dtype=torch.long) for item in value]\n",
        "        elif isinstance(value, bool):\n",
        "            tensor_features[key] = torch.tensor(int(value), dtype=torch.long)\n",
        "        else:\n",
        "            tensor_features[key] = torch.tensor(value, dtype=torch.long)\n",
        "    return tensor_features\n",
        "\n",
        "# print(features)\n",
        "# tensor_features = convert_features_to_tensor(features)\n",
        "# tensor_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj73RNJD59uS"
      },
      "outputs": [],
      "source": [
        "# prompt: write pytorch model that takes as input list[list[number]] and return a single number (max-polling)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TokensListsToNumber(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TokensListsToNumber, self).__init__()\n",
        "        self.linear = nn.Linear(1,1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, subdomains):\n",
        "        # Process each feature individually and then combine\n",
        "        subdomain_values = []\n",
        "        for subdomain in subdomains:\n",
        "          token_values = []\n",
        "          for token in subdomain:\n",
        "              output = self.linear(token.unsqueeze(0).float())\n",
        "              token_values .append(output)\n",
        "          # Get max value across tokens within a subdomain\n",
        "          if token_values:  # Check if token_values is not empty\n",
        "              max_token_value = torch.max(torch.cat(token_values))\n",
        "              subdomain_values.append(max_token_value)\n",
        "\n",
        "        if not subdomain_values:\n",
        "            return torch.tensor([0.0])  # Return a default value\n",
        "\n",
        "        # Find the highest value across all subdomains\n",
        "        max_subdomain_value = torch.max(torch.stack(subdomain_values))\n",
        "\n",
        "        # Apply Sigmoid activation to the highest value\n",
        "        capped_val = self.sigmoid(max_subdomain_value)\n",
        "        return capped_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvAeyraKF5o5"
      },
      "outputs": [],
      "source": [
        "class TokensToNumber(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TokensToNumber, self).__init__()\n",
        "        self.linear = nn.Linear(1,1, bias=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        token_values = []\n",
        "        for token in tokens:\n",
        "            output = self.linear(token.unsqueeze(0).float())\n",
        "            token_values.append(output)\n",
        "\n",
        "        if not token_values:\n",
        "            return torch.tensor([0.0])  # Return a default value\n",
        "\n",
        "        # Find the highest value across all tokens\n",
        "        max_token_value = torch.max(torch.cat(token_values))\n",
        "\n",
        "        # Apply Sigmoid activation to the highest value\n",
        "        capped_val = self.sigmoid(max_token_value)\n",
        "        return capped_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5eosfogHGHf"
      },
      "outputs": [],
      "source": [
        "# tensor_features['domain'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbgf5JM7MDhy"
      },
      "outputs": [],
      "source": [
        "# prompt: create pytorch phishing clasiffaier ligth model- input: tensor of size 16\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PhishingClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PhishingClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(9, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 1)  # Output size 1 (binary classification)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "phishingClassifierModel = PhishingClassifier()\n",
        "input_tensor = torch.randn(1, 9)  # Example input tensor\n",
        "output = phishingClassifierModel(input_tensor)\n",
        "output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mPPqqsRMAcl"
      },
      "outputs": [],
      "source": [
        "# # prompt: display the weigths of model\n",
        "\n",
        "# # Print model weights\n",
        "# for name, param in phishingClassifierModel.named_parameters():\n",
        "#     if param.requires_grad:\n",
        "#         # print(name, param.data)\n",
        "#         print(name, param.data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM6WgrJFGkGE"
      },
      "outputs": [],
      "source": [
        "subdomainsToNumberModel = TokensListsToNumber()\n",
        "domainModel = TokensToNumber()\n",
        "tldModel = TokensToNumber()\n",
        "pathModel = TokensToNumber()\n",
        "queryModel = TokensToNumber()\n",
        "phishingClassifierModel = PhishingClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w_1ftiYT-lj"
      },
      "outputs": [],
      "source": [
        "phishingClassifierModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishingClassifierModel_path_exclude.pth'))\n",
        "subdomainsToNumberModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/subdomainsToNumberModel_path_exclude.pth'))\n",
        "domainModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/domainModel_path_exclude.pth'))\n",
        "tldModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/tldModel_path_exclude.pth'))\n",
        "pathModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/pathModel_path_exclude.pth'))\n",
        "queryModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/queryModel_path_exclude.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59U18GkXTGAO"
      },
      "source": [
        "# Model training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXbf4ZTrGf1l"
      },
      "outputs": [],
      "source": [
        "# # Example usage (assuming you have a vocab_size and embedding_dim defined)\n",
        "\n",
        "# # Example input\n",
        "# url = \"https://user:password@www.example.co.uk:8080/path/to/page?q=query#fragment\"\n",
        "# features = extract_url_features(url)\n",
        "# tensor_features = convert_features_to_tensor(features)\n",
        "\n",
        "# # Make a prediction\n",
        "# subdomain_output = subdomainsToNumberModel(tensor_features['subdomains'])\n",
        "# domain_output = domainModel(tensor_features['domain'])\n",
        "# tld_output = tldModel(tensor_features['tld'])\n",
        "# path_output = pathModel(tensor_features['path'])\n",
        "# query_output = queryModel(tensor_features['query'])\n",
        "\n",
        "# subdomain_output, domain_output, tld_output, path_output, query_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3sOMP4aV-ZI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "phishingClassifierModel.to(device)\n",
        "subdomainsToNumberModel.to(device)\n",
        "domainModel.to(device)\n",
        "tldModel.to(device)\n",
        "pathModel.to(device)\n",
        "queryModel.to(device)\n",
        "\n",
        "# Loss function\n",
        "loss_function = nn.BCELoss(reduction='none')\n",
        "# Combine all parameters into one optimizer\n",
        "optimizer = optim.Adam(\n",
        "    list(phishingClassifierModel.parameters()) +\n",
        "    list(subdomainsToNumberModel.parameters()) +\n",
        "    list(domainModel.parameters()) +\n",
        "    list(tldModel.parameters()) +\n",
        "    list(pathModel.parameters()) +\n",
        "    list(queryModel.parameters()),\n",
        "    lr=0.0001\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzyNcPtyaQvN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the DataFrame into training (90%) and testing (10%) sets\n",
        "train_df, test_df = train_test_split(\n",
        "    final_merged_df,\n",
        "    test_size=0.01,\n",
        "    stratify=final_merged_df['type'],  # Maintain class proportions\n",
        "    random_state=42  # For reproducibility\n",
        ")\n",
        "\n",
        "# Display the shapes of the resulting DataFrames\n",
        "print(\"Train DataFrame shape:\", train_df.shape)\n",
        "print(\"Test DataFrame shape:\", test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAp-FmeyWeJU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.choice(['phishing','legitimate'], p=[0.2,0.8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps1DutBKtCsA"
      },
      "outputs": [],
      "source": [
        "# prompt: check performance on test_df\n",
        "\n",
        "# phishingClassifierModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishingClassifierModel.pth'))\n",
        "# subdomainsToNumberModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/subdomainsToNumberModel.pth'))\n",
        "# domainModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/domainModel.pth'))\n",
        "# tldModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/tldModel.pth'))\n",
        "# pathModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/pathModel.pth'))\n",
        "# queryModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/queryModel.pth'))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def test_acc(df_to_test):\n",
        "  # Set models to evaluation mode\n",
        "  phishingClassifierModel.eval()\n",
        "  subdomainsToNumberModel.eval()\n",
        "  domainModel.eval()\n",
        "  tldModel.eval()\n",
        "  pathModel.eval()\n",
        "  queryModel.eval()\n",
        "\n",
        "  # Set device\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Move models to device\n",
        "  phishingClassifierModel.to(device)\n",
        "  subdomainsToNumberModel.to(device)\n",
        "  domainModel.to(device)\n",
        "  tldModel.to(device)\n",
        "  pathModel.to(device)\n",
        "  queryModel.to(device)\n",
        "\n",
        "\n",
        "  correct_predictions = 0\n",
        "  total_samples = 0\n",
        "  predicted_lables = []\n",
        "  true_labels = []\n",
        "  with torch.no_grad():  # Disable gradient calculations for evaluation\n",
        "      for index, row in df_to_test.iterrows():\n",
        "          try:\n",
        "              url = row['url']\n",
        "              true_label = 1 if row['type'] == 'phishing' else 0\n",
        "\n",
        "              features = extract_url_features(url)\n",
        "              tensor_features = convert_features_to_tensor(features)\n",
        "\n",
        "              for key in tensor_features:\n",
        "                  if isinstance(tensor_features[key], torch.Tensor):\n",
        "                      tensor_features[key] = tensor_features[key].to(device)\n",
        "\n",
        "              subdomain_output = subdomainsToNumberModel(tensor_features['subdomains'])\n",
        "              domain_output = domainModel(tensor_features['domain'])\n",
        "              tld_output = tldModel(tensor_features['tld'])\n",
        "              # path_output = pathModel(tensor_features['path'])\n",
        "              # query_output = queryModel(tensor_features['query'])\n",
        "\n",
        "              vector = [\n",
        "                  tensor_features['protocol'],\n",
        "                  tensor_features['hasAuth'],\n",
        "                  subdomain_output,\n",
        "                  domain_output,\n",
        "                  tld_output,\n",
        "                  # path_output,\n",
        "                  # query_output,\n",
        "                  tensor_features['port'],\n",
        "                  # tensor_features['pathLength'],\n",
        "                  # tensor_features['queryLength'],\n",
        "                  # tensor_features['urlLength'],\n",
        "                  # tensor_features['hasAtOrTildSymbol'],\n",
        "                  tensor_features['hasDoubleSlash'],\n",
        "                  # tensor_features['hyphenCount'],\n",
        "                  tensor_features['numbersInSubdomains'],\n",
        "                  # tensor_features['hasDomainLikeInPath'],\n",
        "                  tensor_features['numOfSubdomains']\n",
        "              ]\n",
        "              features_vector = torch.tensor(vector, dtype=torch.float32).to(device)\n",
        "              prediction = phishingClassifierModel(features_vector.unsqueeze(0)).item()\n",
        "              predicted_label = 1 if prediction >= 0.5 else 0\n",
        "              predicted_lables.append(predicted_label)\n",
        "              true_labels.append(true_label)\n",
        "\n",
        "              if predicted_label == true_label:\n",
        "                  correct_predictions += 1\n",
        "              total_samples += 1\n",
        "          except Exception as e:\n",
        "              print(f\"Error processing URL: {url}, Error: {e}\")\n",
        "\n",
        "  accuracy = correct_predictions / total_samples if total_samples > 0 else 0\n",
        "  print(f\"Accuracy on test data: {accuracy:.4f}\")\n",
        "\n",
        "  # Calculate metrics\n",
        "  accuracy = accuracy_score(true_labels, predicted_lables)\n",
        "  precision = precision_score(true_labels, predicted_lables)\n",
        "  recall = recall_score(true_labels, predicted_lables)\n",
        "  f1 = f1_score(true_labels, predicted_lables)\n",
        "  confusion_matrix_val = confusion_matrix(true_labels, predicted_lables)\n",
        "  # Print the metrics\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"Precision: {precision:.4f}\")\n",
        "  print(f\"Recall: {recall:.4f}\")\n",
        "  print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix_val)\n",
        "\n",
        "  phishingClassifierModel.train()\n",
        "  subdomainsToNumberModel.train()\n",
        "  domainModel.train()\n",
        "  tldModel.train()\n",
        "  pathModel.train()\n",
        "  queryModel.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0kGSHQ0W9E2"
      },
      "outputs": [],
      "source": [
        "test_acc(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oqB5EietL3iM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Ensure all models are in training mode\n",
        "phishingClassifierModel.train()\n",
        "subdomainsToNumberModel.train()\n",
        "domainModel.train()\n",
        "tldModel.train()\n",
        "pathModel.train()\n",
        "queryModel.train()\n",
        "\n",
        "batch_size = 32\n",
        "epochs_num = 10\n",
        "\n",
        "phishing_samples = train_df[train_df['type'] == 'phishing'].sample(frac=1).reset_index(drop=True)\n",
        "legit_samples = train_df[train_df['type'] != 'phishing'].sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "for epoch in range(epochs_num):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs_num}\")\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~ONLY PHISHING~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "    test_acc(phishing_df_new)\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~SPLITED TEST~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "    test_acc(test_df)\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "\n",
        "    train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    i = 0\n",
        "    while i < len(train_df) * 2:\n",
        "        final_vectors_batch = []\n",
        "        labels_batch = []\n",
        "        samples_in_batch = 0\n",
        "\n",
        "        # Try to build a complete batch\n",
        "        while samples_in_batch < batch_size:\n",
        "            choose_type = np.random.choice(['phishing','legitimate'], p=[0.23,0.77])\n",
        "            if choose_type == 'phishing':\n",
        "              j = np.random.randint(0, len(phishing_samples))\n",
        "              sample = phishing_samples.iloc[j]\n",
        "            else:\n",
        "              j = np.random.randint(0, len(legit_samples))\n",
        "              sample = legit_samples.iloc[j]\n",
        "            i += 1\n",
        "\n",
        "            try:\n",
        "                url = sample.url\n",
        "                label = 1 if sample.type == 'phishing' else 0\n",
        "                features = extract_url_features(url)\n",
        "                tensor_features = convert_features_to_tensor(features)\n",
        "\n",
        "                # Move input tensors to device\n",
        "                for key in tensor_features:\n",
        "                    if isinstance(tensor_features[key], torch.Tensor):\n",
        "                        tensor_features[key] = tensor_features[key].to(device)\n",
        "\n",
        "                # Sub-model predictions\n",
        "                subdomain_output = subdomainsToNumberModel(tensor_features['subdomains'])\n",
        "                domain_output = domainModel(tensor_features['domain'])\n",
        "                tld_output = tldModel(tensor_features['tld'])\n",
        "                # path_output = pathModel(tensor_features['path'])\n",
        "                # query_output = queryModel(tensor_features['query'])\n",
        "\n",
        "                vector = [\n",
        "                    tensor_features['protocol'],\n",
        "                    tensor_features['hasAuth'],\n",
        "                    subdomain_output,\n",
        "                    domain_output,\n",
        "                    tld_output,\n",
        "                    # path_output,\n",
        "                    # query_output,\n",
        "                    tensor_features['port'],\n",
        "                    # tensor_features['pathLength'],\n",
        "                    # tensor_features['queryLength'],\n",
        "                    # tensor_features['urlLength'],\n",
        "                    # tensor_features['hasAtOrTildSymbol'],\n",
        "                    tensor_features['hasDoubleSlash'],\n",
        "                    # tensor_features['hyphenCount'],\n",
        "                    tensor_features['numbersInSubdomains'],\n",
        "                    # tensor_features['hasDomainLikeInPath'],\n",
        "                    tensor_features['numOfSubdomains']\n",
        "                ]\n",
        "\n",
        "                features_vector = torch.tensor(vector, dtype=torch.float32).to(device)\n",
        "                label_tensor = torch.tensor([label], dtype=torch.float32).to(device)\n",
        "\n",
        "                final_vectors_batch.append(features_vector)\n",
        "                labels_batch.append(label_tensor)\n",
        "                samples_in_batch += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping sample at index {i - 1} due to error: {e}\")\n",
        "\n",
        "        if not final_vectors_batch:\n",
        "            continue  # skip batch if nothing valid\n",
        "\n",
        "        # Batch tensor stack\n",
        "        final_vectors_batch = torch.stack(final_vectors_batch)\n",
        "        labels_batch = torch.stack(labels_batch).squeeze()\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = phishingClassifierModel(final_vectors_batch).squeeze()\n",
        "\n",
        "        # Compute loss\n",
        "        losses = loss_function(predictions, labels_batch)\n",
        "        weights = torch.where(labels_batch == 1, torch.tensor(10.0).to(device), torch.tensor(1.0).to(device)) # option for imbalanced- currently balanced\n",
        "        weighted_losses = weights * losses\n",
        "        loss = weighted_losses.mean()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if i % 1000 == 0:\n",
        "          print(f\"Processed Batch {i // batch_size}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = epoch_loss / (len(train_df) / batch_size)\n",
        "    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\")\n",
        "    torch.save(phishingClassifierModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishingClassifierModel_more_data_t.pth')\n",
        "    torch.save(subdomainsToNumberModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/subdomainsToNumberModel_more_data_t.pth')\n",
        "    torch.save(domainModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/domainModel_more_data_t.pth')\n",
        "    torch.save(tldModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/tldModel_more_data_t.pth')\n",
        "    torch.save(pathModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/pathModel_more_data_t.pth')\n",
        "    torch.save(queryModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/queryModel_more_data_t.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOC2tE9fs5zA"
      },
      "outputs": [],
      "source": [
        "# prompt: save current models\n",
        "\n",
        "# Save the model's state_dict\n",
        "torch.save(phishingClassifierModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishingClassifierModel_path_exclude.pth')\n",
        "torch.save(subdomainsToNumberModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/subdomainsToNumberModel_path_exclude.pth')\n",
        "torch.save(domainModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/domainModel_path_exclude.pth')\n",
        "torch.save(tldModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/tldModel_path_exclude.pth')\n",
        "torch.save(pathModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/pathModel_path_exclude.pth')\n",
        "torch.save(queryModel.state_dict(), '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/queryModel_path_exclude.pth')\n",
        "\n",
        "print(\"Models saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuSY2oOeSMkL"
      },
      "source": [
        "## testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8uwIvogQl5X"
      },
      "outputs": [],
      "source": [
        "test_acc(phishing_df_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hE4nFNNOGkD"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# # Calculate metrics\n",
        "# accuracy = accuracy_score(true_labels, predicted_lables)\n",
        "# precision = precision_score(true_labels, predicted_lables)\n",
        "# recall = recall_score(true_labels, predicted_lables)\n",
        "# f1 = f1_score(true_labels, predicted_lables)\n",
        "# confusion_matrix_val = confusion_matrix(true_labels, predicted_lables)\n",
        "# # Print the metrics\n",
        "# print(f\"Accuracy: {accuracy:.4f}\")\n",
        "# print(f\"Precision: {precision:.4f}\")\n",
        "# print(f\"Recall: {recall:.4f}\")\n",
        "# print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# print(\"Confusion Matrix:\")\n",
        "# print(confusion_matrix_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoDUTbpPttTC"
      },
      "outputs": [],
      "source": [
        "# prompt: draw heatmap of the important of each feature (by the model weigths)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'phishingClassifierModel' is your loaded model\n",
        "weights = phishingClassifierModel.fc1.weight.cpu().detach().numpy()  # Get the weights of the first fully connected layer\n",
        "\n",
        "# Feature names (replace with your actual feature names)\n",
        "feature_names = [\"protocol\", \"hasAuth\", \"subdomains\", \"domain\", \"tld\",\n",
        "                #  \"path\", \"query\",\n",
        "                 \"port\",\n",
        "                #  \"pathLength\", \"queryLength\", \"urlLength\",\n",
        "                 \"hasAtOrTildSymbol\", \"hasDoubleSlash\", \"hyphenCount\", 'numbersInSubdomains', 'hasDomainLikeInPath', 'numOfSubdomains']\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(np.abs(weights), annot=True, fmt=\".2f\", cmap=\"YlGnBu\", xticklabels=feature_names, yticklabels=range(weights.shape[0])) # Use absolute values for better visualization\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Hidden Neurons\")\n",
        "plt.title(\"Feature Importance (Absolute Weights of First Layer)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yXumJerTa6T"
      },
      "source": [
        "# create features csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxTRnVB3O9d1"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl1LyEtEv5xI"
      },
      "outputs": [],
      "source": [
        "# prompt: create feature-vector dataframe (convert urls to features as creates in the training loop)\n",
        "\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from urllib.parse import urlparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# phishingClassifierModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishingClassifierModel.pth'))\n",
        "# subdomainsToNumberModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/subdomainsToNumberModel.pth'))\n",
        "# domainModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/domainModel.pth'))\n",
        "# tldModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/tldModel.pth'))\n",
        "# pathModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/pathModel.pth'))\n",
        "# queryModel.load_state_dict(torch.load('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/queryModel.pth'))\n",
        "\n",
        "# Set models to evaluation mode\n",
        "phishingClassifierModel.eval()\n",
        "subdomainsToNumberModel.eval()\n",
        "domainModel.eval()\n",
        "tldModel.eval()\n",
        "pathModel.eval()\n",
        "queryModel.eval()\n",
        "\n",
        "# Set device (same as training)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "phishingClassifierModel.to(device)\n",
        "subdomainsToNumberModel.to(device)\n",
        "domainModel.to(device)\n",
        "tldModel.to(device)\n",
        "pathModel.to(device)\n",
        "queryModel.to(device)\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def create_feature_vector_df(dataframe):\n",
        "    \"\"\"\n",
        "    Creates a DataFrame with feature vectors for a list of URLs.\n",
        "    \"\"\"\n",
        "    final_vectors_all = []\n",
        "    labels_all = []\n",
        "    for i in range(len(dataframe)):\n",
        "      try:\n",
        "          if (i % 10000 == 0):\n",
        "            print(i)\n",
        "          sample = dataframe.iloc[i]\n",
        "\n",
        "          url = sample.url\n",
        "          label = 1 if sample.type == 'phishing' else 0\n",
        "          features = extract_url_features(url)\n",
        "          tensor_features = convert_features_to_tensor(features)\n",
        "\n",
        "          # Move input tensors to device\n",
        "          for key in tensor_features:\n",
        "              if isinstance(tensor_features[key], torch.Tensor):\n",
        "                  tensor_features[key] = tensor_features[key].to(device)\n",
        "\n",
        "          # Sub-model predictions\n",
        "          subdomain_output = subdomainsToNumberModel(tensor_features['subdomains'])\n",
        "          domain_output = domainModel(tensor_features['domain'])\n",
        "          tld_output = tldModel(tensor_features['tld'])\n",
        "          path_output = pathModel(tensor_features['path'])\n",
        "          query_output = queryModel(tensor_features['query'])\n",
        "\n",
        "          vector = [\n",
        "              tensor_features['protocol'],\n",
        "              tensor_features['hasAuth'],\n",
        "              subdomain_output,\n",
        "              domain_output,\n",
        "              tld_output,\n",
        "              # path_output,\n",
        "              # query_output,\n",
        "              tensor_features['port'],\n",
        "              # tensor_features['pathLength'],\n",
        "              # tensor_features['queryLength'],\n",
        "              # tensor_features['urlLength'],\n",
        "              tensor_features['hasAtOrTildSymbol'],\n",
        "              tensor_features['hasDoubleSlash'],\n",
        "              tensor_features['hyphenCount'],\n",
        "              tensor_features['numbersInSubdomains'],\n",
        "              torch.tensor(0.0, dtype=torch.long), #tensor_features['hasDomainLikeInPath'],\n",
        "              tensor_features['numOfSubdomains']\n",
        "          ]\n",
        "\n",
        "          features_vector_numpy = torch.tensor(vector, dtype=torch.float32).cpu().numpy()\n",
        "\n",
        "          final_vectors_all.append(features_vector_numpy)\n",
        "          labels_all.append(label)\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Skipping sample at index {i - 1} due to error: {e}\")\n",
        "\n",
        "    return [final_vectors_all, labels_all]\n",
        "# Example usage\n",
        "# final_vectors_all, labels_all = create_feature_vector_df(final_merged_df)\n",
        "final_vectors_all, labels_all = create_feature_vector_df(phishing_df_new)\n",
        "# feature_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSUgmX8-7jRe"
      },
      "outputs": [],
      "source": [
        "features_df = pd.DataFrame({'features_vector': final_vectors_all, 'labels': labels_all})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNGJ6njDS9_b"
      },
      "outputs": [],
      "source": [
        "phishing_vectors_all, phishing_labels_all = create_feature_vector_df(phishing_df_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vyM4YNKTIQX"
      },
      "outputs": [],
      "source": [
        "phishing_featurs_df = pd.DataFrame({'features_vector': phishing_vectors_all, 'labels': phishing_labels_all})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFxDF4n5zS0D"
      },
      "outputs": [],
      "source": [
        "final_merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0-72CvvT0ZJ"
      },
      "outputs": [],
      "source": [
        "features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O2rtN6pVZ5s"
      },
      "outputs": [],
      "source": [
        "phishing_featurs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf4fTd5pVSMd"
      },
      "outputs": [],
      "source": [
        "# prompt: save featurs_df\n",
        "\n",
        "# Save the features DataFrame to a CSV file\n",
        "features_df.to_csv('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/features.csv', index=False)\n",
        "\n",
        "# Save the phishing features DataFrame to a CSV file\n",
        "# phishing_featurs_df.to_csv('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishing_features.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9nQAVoNTr_w"
      },
      "source": [
        "# Use features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn4s9qvQVQ2s"
      },
      "outputs": [],
      "source": [
        "# prompt: load '/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/features.csv' to dataframe featurs_df\n",
        "\n",
        "import pandas as pd\n",
        "features_df = pd.read_csv('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/features.csv')\n",
        "\n",
        "# phishing_featurs_df = pd.read_csv('/content/drive/MyDrive/Ofir/CyberAI/Phishing/URLs_Dataset/phishing_features.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChvpvsRqUDEd"
      },
      "outputs": [],
      "source": [
        "features_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj8km3xFvjOT"
      },
      "outputs": [],
      "source": [
        "# prompt: features_df is a dataframe with 'features_vector' (14 values float array) and lables (0,1). split to train and test, train logistic regression and plot performance oon test\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assuming features_df is already loaded as shown in the provided code\n",
        "\n",
        "# Convert the 'features_vector' column from string to numpy array\n",
        "# n_features_df = phishing_featurs_df.copy()\n",
        "n_features_df = features_df.copy()\n",
        "# n_features_df['features_vector'] = features_df['features_vector'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
        "# n_features_df['features_vector'] = phishing_featurs_df['features_vector'].apply(lambda x: np.fromstring(x[1:-1], sep=' '))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    np.stack(n_features_df['features_vector'].values),  # Use np.stack to create a 2D array\n",
        "    n_features_df['labels'],\n",
        "    test_size=0.99,\n",
        "    random_state=42\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWsy_Ecmfj6E"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "base_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/Model/'\n",
        "logreg = joblib.load(base_path + 'logreg.pkl')\n",
        "dt_classifier = joblib.load(base_path + 'dt_classifier.pkl')\n",
        "rf_classifier = joblib.load(base_path + 'rf_classifier.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwTcju9dYots"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Ara3eXUp90"
      },
      "source": [
        "## LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAj5k5ySUirA"
      },
      "outputs": [],
      "source": [
        "# Train a Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "logreg.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYwioXKUUt2L"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['legitimate', 'Phishing'],\n",
        "            yticklabels=['legitimate', 'Phishing'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yln-5aWQV0XC"
      },
      "source": [
        "# DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXUj7ou1V6yM"
      },
      "outputs": [],
      "source": [
        "# prompt: train decision tree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)  # You can adjust hyperparameters here\n",
        "dt_classifier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7LXdYt-WDPt"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_dt = dt_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Decision Tree Accuracy: {accuracy_dt}\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# Plot confusion matrix for Decision Tree\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['legitimate', 'Phishing'],\n",
        "            yticklabels=['legitimate', 'Phishing'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix (Decision Tree)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8NbOW3pWL9Q"
      },
      "source": [
        "# forest tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnsh9LY_WHZM"
      },
      "outputs": [],
      "source": [
        "# prompt: train forest tree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42, n_estimators=10)  # You can adjust hyperparameters here\n",
        "rf_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iPUsPvyWUX_"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Plot confusion matrix for Random Forest\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['legitimate', 'Phishing'],\n",
        "            yticklabels=['legitimate', 'Phishing'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix (Random Forest)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiT3ZYEV2yQv"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "base_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/Model/'\n",
        "joblib.dump(logreg, base_path + 'logreg.pkl')\n",
        "joblib.dump(dt_classifier, base_path + 'dt_classifier.pkl')\n",
        "joblib.dump(rf_classifier, base_path + 'rf_classifier.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZIeGwwH6fFi"
      },
      "outputs": [],
      "source": [
        "rf_classifier = joblib.load(base_path + 'rf_classifier.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXMPBjfRXVPs"
      },
      "source": [
        "# check example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke7s5oBSXbJp"
      },
      "outputs": [],
      "source": [
        "# prompt: create feature-vector dataframe (convert urls to features as creates in the training loop)\n",
        "\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from urllib.parse import urlparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set models to evaluation mode\n",
        "phishingClassifierModel.eval()\n",
        "subdomainsToNumberModel.eval()\n",
        "domainModel.eval()\n",
        "tldModel.eval()\n",
        "pathModel.eval()\n",
        "queryModel.eval()\n",
        "\n",
        "# Set device (same as training)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "phishingClassifierModel.to(device)\n",
        "subdomainsToNumberModel.to(device)\n",
        "domainModel.to(device)\n",
        "tldModel.to(device)\n",
        "pathModel.to(device)\n",
        "queryModel.to(device)\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def create_feature_vector_from_urls(urls):\n",
        "    \"\"\"\n",
        "    Creates a DataFrame with feature vectors for a list of URLs.\n",
        "    \"\"\"\n",
        "    final_vectors_all = []\n",
        "    for i in range(len(urls)):\n",
        "      try:\n",
        "          url = urls[i]\n",
        "          features = extract_url_features(url)\n",
        "          # print(features)\n",
        "          tensor_features = convert_features_to_tensor(features)\n",
        "\n",
        "          # Move input tensors to device\n",
        "          for key in tensor_features:\n",
        "              if isinstance(tensor_features[key], torch.Tensor):\n",
        "                  tensor_features[key] = tensor_features[key].to(device)\n",
        "\n",
        "          # Sub-model predictions\n",
        "          subdomain_output = subdomainsToNumberModel(tensor_features['subdomains'])\n",
        "          domain_output = domainModel(tensor_features['domain'])\n",
        "          tld_output = tldModel(tensor_features['tld'])\n",
        "          path_output = pathModel(tensor_features['path'])\n",
        "          query_output = queryModel(tensor_features['query'])\n",
        "\n",
        "          vector = [\n",
        "              tensor_features['protocol'],\n",
        "              tensor_features['hasAuth'],\n",
        "              subdomain_output,\n",
        "              domain_output,\n",
        "              tld_output,\n",
        "              # path_output,\n",
        "              # query_output,\n",
        "              tensor_features['port'],\n",
        "              # tensor_features['pathLength'],\n",
        "              # tensor_features['queryLength'],\n",
        "              # tensor_features['urlLength'],\n",
        "              tensor_features['hasAtOrTildSymbol'],\n",
        "              tensor_features['hasDoubleSlash'],\n",
        "              tensor_features['hyphenCount'],\n",
        "              tensor_features['numbersInSubdomains'],\n",
        "              tensor_features['hasDomainLikeInPath'],\n",
        "              tensor_features['numOfSubdomains']\n",
        "          ]\n",
        "\n",
        "          features_vector_numpy = torch.tensor(vector, dtype=torch.float32).cpu().numpy()\n",
        "\n",
        "          final_vectors_all.append(features_vector_numpy)\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Skipping sample at index {i - 1} due to error: {e}\")\n",
        "\n",
        "    return final_vectors_all\n",
        "# feature_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF1y2VtrWWNm"
      },
      "outputs": [],
      "source": [
        "vectors = create_feature_vector_from_urls(['http://accountlockseohive.vercel.app/get_help\t'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tnJCqwnFAE2"
      },
      "outputs": [],
      "source": [
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQK3O53mhwV5"
      },
      "outputs": [],
      "source": [
        "# model = logreg\n",
        "model = dt_classifier\n",
        "# model = rf_classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZsn1lL9hZVD"
      },
      "outputs": [],
      "source": [
        "model.predict_proba(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FJklKEJ-_7U"
      },
      "outputs": [],
      "source": [
        "phishingClassifierModel.eval()\n",
        "modelPhish = phishingClassifierModel\n",
        "tensorVectors = torch.tensor(vectors, dtype=torch.float32).to(device)\n",
        "modelPhish.forward(tensorVectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPZL9UiwhjAh"
      },
      "outputs": [],
      "source": [
        "model.predict_proba(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1enbsO7Uk-FD"
      },
      "outputs": [],
      "source": [
        "vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLx0QNiHXBY5"
      },
      "source": [
        "# export weigths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFLHpzBgiSJd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFBjbOW2iGuX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "\n",
        "base_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/Model/'\n",
        "def export_tokens_to_number(model, file_name):\n",
        "    weights = {\n",
        "        'linear_weight': model.linear.weight.detach().numpy().tolist(),\n",
        "        'linear_bias': model.linear.bias.detach().numpy().tolist()\n",
        "    }\n",
        "    print(file_name, weights)\n",
        "    file_path = base_path + file_name\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump(weights, f)\n",
        "\n",
        "export_tokens_to_number(subdomainsToNumberModel, 'subdomains_model.json')\n",
        "export_tokens_to_number(domainModel, 'domain_model.json')\n",
        "export_tokens_to_number(tldModel, 'tld_model.json')\n",
        "export_tokens_to_number(pathModel, 'path_model.json')\n",
        "export_tokens_to_number(queryModel, 'query_model.json')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrR9HzxAipJC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def export_tree(tree, feature_names):\n",
        "    tree_ = tree.tree_\n",
        "\n",
        "    def recurse(node):\n",
        "        if tree_.feature[node] != -2:  # Not a leaf\n",
        "            return {\n",
        "                \"feature\": feature_names[tree_.feature[node]],\n",
        "                \"threshold\": tree_.threshold[node],\n",
        "                \"left\": recurse(tree_.children_left[node]),\n",
        "                \"right\": recurse(tree_.children_right[node])\n",
        "            }\n",
        "        else:\n",
        "            return {\"value\": tree_.value[node].tolist()}\n",
        "\n",
        "    return recurse(0)\n",
        "\n",
        "# Get feature names (assuming X_train is a pandas DataFrame)\n",
        "feature_names = ['protocol', 'hasAuth', 'subdomains', 'domain', 'tld', 'path', 'query', 'port', 'pathLength', 'queryLength', 'urlLength', 'hasAtOrTildSymbol', 'hasDoubleSlash', 'hyphenCount']\n",
        "\n",
        "# Export all trees\n",
        "forest_json = [export_tree(estimator, feature_names) for estimator in rf_classifier.estimators_]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd-rmdBKqt0S"
      },
      "outputs": [],
      "source": [
        "sitemap_url = f\"https://google.com/sitemap.xml\"\n",
        "response = requests.get(sitemap_url, timeout=5)\n",
        "soup = BeautifulSoup(response.text, 'xml')\n",
        "[loc.text for loc in soup.find_all('loc')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AGVtcRmpbsM"
      },
      "outputs": [],
      "source": [
        "# Save as JSON\n",
        "with open(base_path + \"random_forest_model.json\", \"w\") as f:\n",
        "    json.dump(forest_json, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViuXRZSquVMG"
      },
      "outputs": [],
      "source": [
        "# prompt: export and save to json dt_classifier as well\n",
        "\n",
        "import json\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Export the Decision Tree classifier\n",
        "def export_tree(tree, feature_names):\n",
        "    tree_ = tree.tree_\n",
        "\n",
        "    def recurse(node):\n",
        "        if tree_.feature[node] != -2:  # Not a leaf\n",
        "            return {\n",
        "                \"feature\": feature_names[tree_.feature[node]],\n",
        "                \"threshold\": tree_.threshold[node],\n",
        "                \"left\": recurse(tree_.children_left[node]),\n",
        "                \"right\": recurse(tree_.children_right[node])\n",
        "            }\n",
        "        else:\n",
        "            return {\"value\": tree_.value[node].tolist()}\n",
        "\n",
        "    return recurse(0)\n",
        "\n",
        "# Assuming you have feature_names defined\n",
        "feature_names = ['protocol', 'hasAuth', 'subdomains', 'domain', 'tld',\n",
        "                 # 'path', 'query',\n",
        "                 'port',\n",
        "                #  'pathLength', 'queryLength', 'urlLength',\n",
        "                 'hasAtOrTildSymbol', 'hasDoubleSlash', 'hyphenCount',\n",
        "                  'numbersInSubdomains', 'hasDomainLikeInPath','numOfSubdomains']\n",
        "\n",
        "dt_classifier_json = export_tree(dt_classifier, feature_names)\n",
        "\n",
        "\n",
        "# Save dt_classifier as JSON\n",
        "base_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/Model/'\n",
        "with open(base_path + \"decision_tree_model.json\", \"w\") as f:\n",
        "    json.dump(dt_classifier_json, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lfZCdnl-u9Hn"
      },
      "outputs": [],
      "source": [
        "dt_classifier_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8W1OQR5wFAv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.tree import _tree\n",
        "\n",
        "def tree_to_dict(tree, feature_names=None):\n",
        "    tree_ = tree.tree_\n",
        "\n",
        "    def recurse(node):\n",
        "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
        "            return {\n",
        "                \"feature\": int(tree_.feature[node]),\n",
        "                \"threshold\": float(tree_.threshold[node]),\n",
        "                \"left\": recurse(tree_.children_left[node]),\n",
        "                \"right\": recurse(tree_.children_right[node]),\n",
        "            }\n",
        "        else:\n",
        "            # leaf node\n",
        "            value = tree_.value[node][0]\n",
        "            return {\n",
        "                \"value\": int(value.argmax())\n",
        "            }\n",
        "\n",
        "    return recurse(0)\n",
        "\n",
        "# Export the tree\n",
        "tree_dict = tree_to_dict(dt_classifier)\n",
        "base_path = '/content/drive/MyDrive/Ofir/CyberAI/Phishing/Model/'\n",
        "\n",
        "with open(base_path + \"decision_tree_model.json\", \"w\") as f:\n",
        "    json.dump(tree_dict, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZNDIa3v_Nkp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}